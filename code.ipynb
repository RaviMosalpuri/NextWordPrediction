{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5154a415",
   "metadata": {},
   "source": [
    "<h1><center>Next Word Prediction using Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313db196",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f3a422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fd822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Metamorphosis_clean.txt\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he foundhimself transformed in his bed into a horrible vermin.\n",
      "He lay on hisarmour-like back, and if he lifted his head a little he could see hisbrown belly, slightly domed and divided by arches into stiff sections.The bedding was hardly able to cover it and seemed ready to slide offany moment.\n",
      "His many legs, pitifully thin compared with the size of therest of him, waved about helplessly as he looked.“What’s happened to me?” he thought.\n",
      "It wasn’t a dream.\n",
      "His room, aproper human room although a little too small, lay peacefully betweenits four familiar walls.\n",
      "A collection of textile samples lay spread outon the table—Samsa was a travelling salesman—and above it there hung apicture that he had recently cut out of an illustrated magazine andhoused in a nice, gilded frame.\n",
      "It showed a lady fitted out with a furhat and fur boa who sat upright, raising a heavy fur muff that coveredthe whole of her lower arm towards the viewer.Gregor then turned to look out the window at the dull weather.\n",
      "Drops ofrain could be heard hitting the pane, which made him feel quite sad.“How about if I sleep a little bit longer and forget all thisnonsense”, he thought, but that was something he was unable to dobecause he was used to sleeping on his right, and in his present statecouldn’t get into that position.\n",
      "However hard he threw himself onto hisright, he always rolled back to where he was.\n",
      "He must have tried it ahundred times, shut his eyes so that he wouldn’t have to look at thefloundering legs, and only stopped when he began to feel a mild, dullpain there that he had never felt before.“Oh, God”, he thought, “what a strenuous career it is that I’ve chosen!Travelling day in and day out.\n",
      "File: TheTrial_clean.txt\n",
      "Someone must have been telling lies about Josef K., he knew he had donenothing wrong but, one morning, he was arrested.\n",
      "Every day at eight inthe morning he was brought his breakfast by Mrs.\n",
      "Grubach's cook--Mrs.Grubach was his landlady--but today she didn't come.\n",
      "That had neverhappened before.\n",
      "K.\n",
      "waited a little while, looked from his pillow at theold woman who lived opposite and who was watching him with aninquisitiveness quite unusual for her, and finally, both hungry anddisconcerted, rang the bell.\n",
      "There was immediately a knock at the doorand a man entered.\n",
      "He had never seen the man in this house before.\n",
      "Hewas slim but firmly built, his clothes were black and close-fitting,with many folds and pockets, buckles and buttons and a belt, all ofwhich gave the impression of being very practical but without making itvery clear what they were actually for.\n",
      "\"Who are you?\" asked K., sittinghalf upright in his bed.\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "directory_path = 'Data'\n",
    "# Get the files in directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Text corpus\n",
    "text_corpus = \"\"\n",
    "# Sentences from corpus\n",
    "sentences_corpus = []\n",
    "\n",
    "# Loop over the files\n",
    "for file in files:\n",
    "    # Print file name\n",
    "    print(f'File: {file}')\n",
    "    # Get the file path\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    # Open the file for reading\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Read the file\n",
    "        text = f.read()\n",
    "        # Remove extra spaces\n",
    "        text = text.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "        # Add file text to text corpus\n",
    "        text_corpus += text\n",
    "        # Split the text into sentences\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "        # Append sentences to corpus\n",
    "        sentences_corpus.append(sentences)\n",
    "        # Display first 10 sentences\n",
    "        for sentence in sentences[:10]:\n",
    "            print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c6c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3836\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Tokenizer from keras\n",
    "tokenizer = Tokenizer()\n",
    "# Fit on the data\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# Get the total words and print\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53, 149], [53, 149, 49], [53, 149, 49, 15], [53, 149, 49, 15, 97], [53, 149, 49, 15, 97, 884]]\n"
     ]
    }
   ],
   "source": [
    "# Create input sequences list\n",
    "input_sequences = []\n",
    "# Loop over lines in all the sentences\n",
    "for line in sentences:\n",
    "    # Convert sentences to tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    # Loop over the token list\n",
    "    for i in range(1, len(token_list)):\n",
    "        # Append the token list to input sequences list\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Print first 5 sequences\n",
    "print(input_sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01464785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum sequence length\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "# Convert the input sequences to matrix\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "# Get the inputs and labels\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "# Convert target data to one-hot encoding\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8983edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# Use Sequential model\n",
    "model = Sequential()\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(input_dim=total_words, output_dim=10, input_length=max_sequence_len-1))\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(128))\n",
    "# Add output layer\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfa75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.0408 - loss: 6.9736\n",
      "Epoch 2/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.0499 - loss: 6.3325\n",
      "Epoch 3/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.0538 - loss: 6.2337\n",
      "Epoch 4/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.0676 - loss: 6.0027\n",
      "Epoch 5/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.0767 - loss: 5.8439\n",
      "Epoch 6/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.0887 - loss: 5.6268\n",
      "Epoch 7/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.0899 - loss: 5.4100\n",
      "Epoch 8/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.1056 - loss: 5.1798\n",
      "Epoch 9/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.1131 - loss: 4.9758\n",
      "Epoch 10/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.1229 - loss: 4.8135\n",
      "Epoch 11/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.1334 - loss: 4.6120\n",
      "Epoch 12/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.1477 - loss: 4.4253\n",
      "Epoch 13/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.1567 - loss: 4.2477\n",
      "Epoch 14/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.1726 - loss: 4.0896\n",
      "Epoch 15/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.1972 - loss: 3.8820\n",
      "Epoch 16/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.2227 - loss: 3.7240\n",
      "Epoch 17/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.2496 - loss: 3.5619\n",
      "Epoch 18/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.2774 - loss: 3.3839\n",
      "Epoch 19/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.3068 - loss: 3.2461\n",
      "Epoch 20/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.3318 - loss: 3.1202\n",
      "Epoch 21/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.3598 - loss: 2.9524\n",
      "Epoch 22/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.3826 - loss: 2.8616\n",
      "Epoch 23/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.4076 - loss: 2.7226\n",
      "Epoch 24/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.4350 - loss: 2.6084\n",
      "Epoch 25/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.4402 - loss: 2.5295\n",
      "Epoch 26/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.4600 - loss: 2.4423\n",
      "Epoch 27/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.4786 - loss: 2.3490\n",
      "Epoch 28/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.4875 - loss: 2.2875\n",
      "Epoch 29/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.5071 - loss: 2.1965\n",
      "Epoch 30/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.5240 - loss: 2.1294\n",
      "Epoch 31/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.5440 - loss: 2.0438\n",
      "Epoch 32/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.5558 - loss: 1.9792\n",
      "Epoch 33/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.5683 - loss: 1.9273\n",
      "Epoch 34/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.5851 - loss: 1.8487\n",
      "Epoch 35/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.5915 - loss: 1.8069\n",
      "Epoch 36/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.6088 - loss: 1.7422\n",
      "Epoch 37/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6220 - loss: 1.6829\n",
      "Epoch 38/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.6336 - loss: 1.6214\n",
      "Epoch 39/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.6497 - loss: 1.5704\n",
      "Epoch 40/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6539 - loss: 1.5339\n",
      "Epoch 41/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6730 - loss: 1.4844\n",
      "Epoch 42/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.6795 - loss: 1.4260\n",
      "Epoch 43/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6876 - loss: 1.3898\n",
      "Epoch 44/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.6976 - loss: 1.3563\n",
      "Epoch 45/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.7104 - loss: 1.3013\n",
      "Epoch 46/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.7158 - loss: 1.2822\n",
      "Epoch 47/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.7265 - loss: 1.2403\n",
      "Epoch 48/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.7329 - loss: 1.1930\n",
      "Epoch 49/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.7369 - loss: 1.1845\n",
      "Epoch 50/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7475 - loss: 1.1381\n",
      "Epoch 51/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.7615 - loss: 1.0753\n",
      "Epoch 52/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7686 - loss: 1.0549\n",
      "Epoch 53/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7741 - loss: 1.0347\n",
      "Epoch 54/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.7859 - loss: 0.9955\n",
      "Epoch 55/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.7852 - loss: 0.9819\n",
      "Epoch 56/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.8023 - loss: 0.9377\n",
      "Epoch 57/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8097 - loss: 0.9089\n",
      "Epoch 58/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8189 - loss: 0.8704\n",
      "Epoch 59/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8221 - loss: 0.8535\n",
      "Epoch 60/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.8294 - loss: 0.8345\n",
      "Epoch 61/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.8352 - loss: 0.8021\n",
      "Epoch 62/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8404 - loss: 0.7797\n",
      "Epoch 63/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8481 - loss: 0.7526\n",
      "Epoch 64/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8493 - loss: 0.7410\n",
      "Epoch 65/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8482 - loss: 0.7275\n",
      "Epoch 66/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8646 - loss: 0.6921\n",
      "Epoch 67/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8704 - loss: 0.6616\n",
      "Epoch 68/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8724 - loss: 0.6492\n",
      "Epoch 69/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.8763 - loss: 0.6197\n",
      "Epoch 70/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8796 - loss: 0.6146\n",
      "Epoch 71/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.8853 - loss: 0.5969\n",
      "Epoch 72/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8858 - loss: 0.5916\n",
      "Epoch 73/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8911 - loss: 0.5652\n",
      "Epoch 74/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.8958 - loss: 0.5573\n",
      "Epoch 75/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8982 - loss: 0.5372\n",
      "Epoch 76/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9061 - loss: 0.5110\n",
      "Epoch 77/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9064 - loss: 0.5055\n",
      "Epoch 78/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9080 - loss: 0.4990\n",
      "Epoch 79/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9127 - loss: 0.4791\n",
      "Epoch 80/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9175 - loss: 0.4646\n",
      "Epoch 81/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9211 - loss: 0.4432\n",
      "Epoch 82/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9237 - loss: 0.4377\n",
      "Epoch 83/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9233 - loss: 0.4370\n",
      "Epoch 84/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9251 - loss: 0.4157\n",
      "Epoch 85/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9238 - loss: 0.4194\n",
      "Epoch 86/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9298 - loss: 0.3957\n",
      "Epoch 87/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9336 - loss: 0.3822\n",
      "Epoch 88/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9335 - loss: 0.3710\n",
      "Epoch 89/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9398 - loss: 0.3660\n",
      "Epoch 90/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9368 - loss: 0.3504\n",
      "Epoch 91/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9383 - loss: 0.3516\n",
      "Epoch 92/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9375 - loss: 0.3467\n",
      "Epoch 93/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9420 - loss: 0.3293\n",
      "Epoch 94/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9473 - loss: 0.3159\n",
      "Epoch 95/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9387 - loss: 0.3394\n",
      "Epoch 96/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9483 - loss: 0.3125\n",
      "Epoch 97/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9524 - loss: 0.2906\n",
      "Epoch 98/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9516 - loss: 0.2911\n",
      "Epoch 99/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9473 - loss: 0.3040\n",
      "Epoch 100/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9493 - loss: 0.2930\n",
      "Epoch 101/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9454 - loss: 0.2971\n",
      "Epoch 102/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9572 - loss: 0.2634\n",
      "Epoch 103/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9530 - loss: 0.2731\n",
      "Epoch 104/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9565 - loss: 0.2607\n",
      "Epoch 105/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9532 - loss: 0.2755\n",
      "Epoch 106/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9532 - loss: 0.2616\n",
      "Epoch 107/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9584 - loss: 0.2434\n",
      "Epoch 108/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9549 - loss: 0.2541\n",
      "Epoch 109/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9585 - loss: 0.2336\n",
      "Epoch 110/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9617 - loss: 0.2310\n",
      "Epoch 111/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9573 - loss: 0.2361\n",
      "Epoch 112/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9548 - loss: 0.2490\n",
      "Epoch 113/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9556 - loss: 0.2457\n",
      "Epoch 114/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9639 - loss: 0.2056\n",
      "Epoch 115/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9682 - loss: 0.1871\n",
      "Epoch 116/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9594 - loss: 0.2145\n",
      "Epoch 117/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.8616 - loss: 0.5314\n",
      "Epoch 118/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9520 - loss: 0.2491\n",
      "Epoch 119/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9661 - loss: 0.1880\n",
      "Epoch 120/120\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9705 - loss: 0.1654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x791948757d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model checkpoint\n",
    "checkpoint = ModelCheckpoint('model_weights.keras', monitor='loss', save_best_only=True, mode='min')\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=120, verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f10e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model('model_weights.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Next predicted words: So why did his sister not even able to make the\n"
     ]
    }
   ],
   "source": [
    "# Input text which will be used to predict next words\n",
    "input_text = \"So why did his sister not\"\n",
    "# Number of next words to predict\n",
    "next_words = 5\n",
    "\n",
    "# Loop over the number of words\n",
    "for _ in range(next_words):\n",
    "    # Get the sequences from text\n",
    "    token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    # Get the pad sequences\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    # Predict the probabilities from model\n",
    "    predicted_probs = model.predict(token_list)\n",
    "    # Get the predicted words\n",
    "    predicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
    "    # Add the predicted word to input text\n",
    "    input_text += \" \" + predicted_word\n",
    "\n",
    "# Print the input and predicted words\n",
    "print(\"Next predicted words:\", input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
